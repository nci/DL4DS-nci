{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6438821e-46a9-4642-9785-b13676de9092",
   "metadata": {},
   "source": [
    "# Empirical downscaling of climate data with DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bacfb44-59b1-4af0-80b8-e32d9f86c3e7",
   "metadata": {},
   "source": [
    "This notebook is modified based on the original DL4DS tutorial notebook to make it working at NCI. It implemented from the following paper, titled: **DL4DS - Deep Learning for empirical DownScaling** <br>\n",
    "Link: https://arxiv.org/pdf/2205.08967.pdf <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42f60f-2745-4b0b-a79a-a4105f3c6f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import ecubevis as ecv\n",
    "import dl4ds as dds\n",
    "import scipy as sp\n",
    "import netCDF4 as nc\n",
    "import climetlab as cml\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc9cc0-06a0-4f5b-b1e0-21e37b7f77c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090118c-947a-4d96-8c84-a249690aa449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change to your working directory\n",
    "import os\n",
    "os.chdir(YOUR_WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a848145-a100-48e6-b511-a096b88d86ce",
   "metadata": {},
   "source": [
    "## MAELSTROM downscaling benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea784d-a98f-481c-8bc6-267e322978c4",
   "metadata": {},
   "source": [
    "**MAchinE Learning for Scalable meTeoROlogy and climate** (MAELSTROM) is a joint project by several climate research organizations that provides various machine learning datasets for climate research. \n",
    "\n",
    "In this notebook, the **MAELSTROM 2m temperature downscaling dataset** has been used. You can downlaod the dataset via cml.load_dataset() method. You can specify the cache directory via the cml.settings.set() method. \n",
    "\n",
    "Or, you can access a copy of dataset directly from Gadi local file system \"/g/data/dk92/apps/dl4ds/data/maelstrom/2m_temperature_downsacaling_dataset\". \n",
    "\n",
    "More details can be found in the following white paper: https://www.maelstrom-eurohpc.eu/content/docs/uploads/doc6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee1be2-f5c6-4d8f-92c5-eb8f441ac5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cml.settings.get(\"cache-directory\") # Find the current cache directory\n",
    "# Change the value of the setting\n",
    "#cml.settings.set(\"cache-directory\", \"/big-disk/climetlab-cache\")\n",
    "# Python kernel restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531243d-861a-42cc-83a7-769245f5b46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download MAELSTROM datasets\n",
    "# # No need to run the following lines at NCI Gadi.\n",
    "#cmlds_train = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"training\")\n",
    "#cmlds_val = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"validation\")\n",
    "#cmlds_test = cml.load_dataset(\"maelstrom-downscaling\", dataset=\"testing\")\n",
    "#cmlds_train.to_xarray().to_netcdf(\"cmlds_train.nc\")\n",
    "#cmlds_val.to_xarray().to_netcdf(\"cmlds_val.nc\")\n",
    "#cmlds_test.to_xarray().to_netcdf(\"cmlds_test.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05387a-cf41-44ef-b468-72a7fe4ada23",
   "metadata": {},
   "source": [
    "### Load dataset From local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca63b1-2c8d-45c7-beff-8623c2374ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmlds_train = cml.load_source('file',\"/g/data/dk92/apps/dl4ds/data/maelstrom/2m_temperature_downsacaling_dataset/cmlds_train.nc\")\n",
    "cmlds_val   = cml.load_source('file',\"/g/data/dk92/apps/dl4ds/data/maelstrom/2m_temperature_downsacaling_dataset/cmlds_val.nc\")\n",
    "cmlds_test  = cml.load_source('file',\"/g/data/dk92/apps/dl4ds/data/maelstrom/2m_temperature_downsacaling_dataset/cmlds_test.nc\")\n",
    "\n",
    "t2m_hr_train = cmlds_train.to_xarray().t2m_tar\n",
    "t2m_hr_test = cmlds_test.to_xarray().t2m_tar\n",
    "t2m_hr_val = cmlds_val.to_xarray().t2m_tar\n",
    "\n",
    "t2m_lr_train = cmlds_train.to_xarray().t2m_in\n",
    "t2m_lr_test = cmlds_test.to_xarray().t2m_in\n",
    "t2m_lr_val = cmlds_val.to_xarray().t2m_in\n",
    "\n",
    "z_hr_train = cmlds_train.to_xarray().z_tar\n",
    "z_hr_test = cmlds_test.to_xarray().z_tar\n",
    "z_hr_val = cmlds_val.to_xarray().z_tar\n",
    "\n",
    "z_lr_train = cmlds_train.to_xarray().z_in\n",
    "z_lr_test = cmlds_test.to_xarray().z_in\n",
    "z_lr_val = cmlds_val.to_xarray().z_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278a16e-eb7c-4244-8594-dc285d4008b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_hr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eecab2-23c7-45f4-b1e3-3187dba2f9df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(t2m_hr_train.shape,t2m_hr_val.shape, t2m_hr_test.shape)\n",
    "print(t2m_lr_train.shape,t2m_lr_val.shape, t2m_lr_test.shape)\n",
    "print(z_hr_train.shape,z_hr_val.shape, z_hr_test.shape)\n",
    "print(z_lr_train.shape,z_lr_val.shape, z_lr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f37c3-236a-45db-8019-7d29bb9359e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The high resolution and low resolution datasets for training are shown below.\n",
    "( ecv.plot(t2m_hr_train, interactive=True) + ecv.plot(t2m_lr_train, interactive=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d9a0b-dca8-4c71-ba4f-11cf5a6bff57",
   "metadata": {},
   "source": [
    "Here we take care of the scaling/normalization of values before training our networks. In this exmample, we center wrt the global mean and scale wrt the global standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4365d-1bbd-4ec9-b7c1-6679236c2bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2m_scaler_train = dds.StandardScaler(axis=None)\n",
    "# Calculate the mean and standard deviation of t2m_hr_train for later scaling.\n",
    "t2m_scaler_train.fit(t2m_hr_train)  \n",
    "# Perform standardization by centering and scaling.\n",
    "y_train = t2m_scaler_train.transform(t2m_hr_train)\n",
    "y_test = t2m_scaler_train.transform(t2m_hr_test)\n",
    "y_val = t2m_scaler_train.transform(t2m_hr_val)\n",
    "\n",
    "x_train = t2m_scaler_train.transform(t2m_lr_train)\n",
    "x_test = t2m_scaler_train.transform(t2m_lr_test)\n",
    "x_val = t2m_scaler_train.transform(t2m_lr_val)\n",
    "\n",
    "z_scaler_train = dds.StandardScaler(axis=None)\n",
    "z_scaler_train.fit(z_hr_train)  \n",
    "y_z_train = z_scaler_train.transform(z_hr_train)\n",
    "y_z_test = z_scaler_train.transform(z_hr_test)\n",
    "y_z_val = z_scaler_train.transform(z_hr_val)\n",
    "\n",
    "x_z_train = z_scaler_train.transform(z_lr_train)\n",
    "x_z_test = z_scaler_train.transform(z_lr_test)\n",
    "x_z_val = z_scaler_train.transform(z_lr_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16692524-0396-4235-a7b7-1793a489701c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecv.plot((x_train[0], y_train[0]), subplot_titles=('t2m coarsened', 't2m highres'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32295bf-311c-413d-b303-4099e1b90213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecv.plot((x_z_train[0], y_z_train[0]), subplot_titles=('geopotential coarsened', 'geopotential highres'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c565d-8ebb-4923-86c6-17a8b80db8f2",
   "metadata": {},
   "source": [
    "Add a unitary dimension called channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591152c7-ef9c-4e35-aecb-d3f4f104aca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = y_train.expand_dims(dim='channel', axis=-1)\n",
    "y_test = y_test.expand_dims(dim='channel', axis=-1)\n",
    "y_val = y_val.expand_dims(dim='channel', axis=-1)\n",
    "\n",
    "x_train = x_train.expand_dims(dim='channel', axis=-1)\n",
    "x_test = x_test.expand_dims(dim='channel', axis=-1)\n",
    "x_val = x_val.expand_dims(dim='channel', axis=-1)\n",
    "\n",
    "y_z_train = y_z_train.expand_dims(dim='channel', axis=-1)\n",
    "y_z_test = y_z_test.expand_dims(dim='channel', axis=-1)\n",
    "y_z_val = y_z_val.expand_dims(dim='channel', axis=-1)\n",
    "\n",
    "x_z_train = x_z_train.expand_dims(dim='channel', axis=-1)\n",
    "x_z_test = x_z_test.expand_dims(dim='channel', axis=-1)\n",
    "x_z_val = x_z_val.expand_dims(dim='channel', axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06959c6-1df5-48b5-baa9-3b50ea1e3334",
   "metadata": {},
   "source": [
    "The resulting shapes of the input arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84ebce-6195-4c2a-bc67-453df4632670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y_train.shape, y_test.shape, y_val.shape)\n",
    "print(x_train.shape, x_test.shape, x_val.shape)\n",
    "\n",
    "print(x_z_train.shape, x_z_test.shape, x_z_val.shape)\n",
    "print(y_z_train.shape, y_z_test.shape, y_z_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76687ad-c066-4e1c-9fa3-5e2c48edb7e7",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d4ead-d994-4322-9081-8ea9b6ec8ca0",
   "metadata": {},
   "source": [
    "For training, DL4DS takes:\n",
    "\n",
    "* HR data [mandatory, highres grid],\n",
    "* LR data [optional, coarse grid],\n",
    "* static variables [optional, highres grid],\n",
    "* time-varyibng variables [optional, coarse to highres grid].\n",
    "\n",
    "In this example, we do not use the LR data because it is just a coarsened version (via interpolation) of the HR t2m and z data. DL4DS carries out this interpolation on the fly by using the helping function dds.create_pair_hr_lr() (not to be called by the user). This process is done automatically inside the training loop (by calling one of the two Trainer classes in DL4DS), which we examplify here with the spc upsampling.\n",
    "\n",
    "The function create_pair_hr_lr() create a pair of HR and LR square sub-patches. In this case, the LR corresponds to a coarsen version of the HR reference with land-ocean mask, topography and auxiliary predictors added as \"image channels\".\n",
    "\n",
    "Parameters:\n",
    "* array (np.ndarray): HR gridded data.\n",
    "* array_lr (np.ndarray): LR gridded data. If not provided, then implicit/coarsened pairs are created from array.\n",
    "* upsampling (str): String with the name of the upsampling method.\n",
    "* scale (int): Scaling factor.\n",
    "* patch_size (int or None): Size of the square patches to be extracted, in pixels for the HR grid.\n",
    "* static_vars (None or list of 2D ndarrays, optional): Static variables such as elevation data or a binary land-ocean mask.\n",
    "* predictors (np.ndarray, optional): Predictor variables in HR. To be concatenated to the LR version of array.\n",
    "* season\n",
    "* debug (bool, optional): If True, plots and debugging information are shown.\n",
    "* interpolation (str, optional): Interpolation used when upsampling/downsampling the training samples. By default 'bicubic'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795c6e3-3228-49a9-a295-0059dcbd2801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = dds.create_pair_hr_lr(y_train.values[0], \n",
    "                          None, \n",
    "                          'spc', \n",
    "                          8, \n",
    "                          None, \n",
    "                          None, \n",
    "                          y_z_train.values[0], \n",
    "                          None, \n",
    "                          True, \n",
    "                          interpolation='inter_area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7d04d-731e-48ac-a6c7-8392b48962e6",
   "metadata": {},
   "source": [
    "To train a model, one need the following data\n",
    "- Must have the **high resolution data**\n",
    "- Can provide the **low resolution data**. However, DL4DS creates the **low  resolution data** using build-in interpolation function, if none is provided. \n",
    "\n",
    "Below is an example of a call to the SupervisedTrainer class to run the training loop for 100 epochs. In this case, we train a network with a ResNet backbone, 8 residual blocks, in post-upsampling via subpixel convolution (implicit training pairs), and a localized convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e8bd2-0be4-4ca3-b0fc-b41ea6e8a4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ARCH_PARAMS = dict(n_filters=8,\n",
    "                   n_blocks=8,\n",
    "                   normalization=None,\n",
    "                   dropout_rate=0.0,\n",
    "                   dropout_variant='spatial',\n",
    "                   attention=False,\n",
    "                   activation='relu',\n",
    "                   localcon_layer=True)\n",
    "\n",
    "trainer = dds.SupervisedTrainer(\n",
    "    backbone='resnet',\n",
    "    upsampling='spc', \n",
    "    data_train=y_train, \n",
    "    data_val=y_val,\n",
    "    data_test=y_test,\n",
    "    data_train_lr=None, \n",
    "    data_val_lr=None,  \n",
    "    data_test_lr=None, \n",
    "    scale=8,\n",
    "    time_window=None, \n",
    "    static_vars=None,\n",
    "    predictors_train=[y_z_train],\n",
    "    predictors_val=[y_z_val],\n",
    "    predictors_test=[y_z_test],\n",
    "    interpolation='inter_area',\n",
    "    patch_size=None, \n",
    "    batch_size=60, \n",
    "    loss='mae',\n",
    "    epochs=100, \n",
    "    steps_per_epoch=None, \n",
    "    validation_steps=None, \n",
    "    test_steps=None, \n",
    "    learning_rate=(1e-3, 1e-4), lr_decay_after=1e4,\n",
    "    early_stopping=False, patience=6, min_delta=0, \n",
    "    save=False, \n",
    "    save_path=None,\n",
    "    show_plot=True, verbose=True, \n",
    "    device='CPU', \n",
    "    **ARCH_PARAMS)\n",
    "\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c60167-f32c-42d1-acfd-f6080892df67",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4cbe2c-36db-4cc7-b4c3-45eb1d96d465",
   "metadata": {},
   "source": [
    "Let's evaluate the results on holdout data -- the test split of the benchmark dataset that has not been used while training (updating the network weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095575b-bfd5-476c-a5b6-2c254696f8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (y_test.shape, y_z_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3f984-33c4-4690-8815-c89cf25853eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pred = dds.Predictor(\n",
    "    trainer, \n",
    "    y_test, \n",
    "    scale=8, \n",
    "    array_in_hr=True,\n",
    "    static_vars=None, \n",
    "    predictors=[y_z_test], \n",
    "    time_window=None,\n",
    "    interpolation='inter_area', \n",
    "    batch_size=8,\n",
    "    scaler=t2m_scaler_train,\n",
    "    save_path=None,\n",
    "    save_fname=None,\n",
    "    return_lr=True,\n",
    "    device='GPU')\n",
    "\n",
    "unscaled_y_pred, coarsened_array = pred.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ad45a-62d7-4f40-a413-d65083a220ef",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac459e0-dc7a-473a-a310-6027a4a4af12",
   "metadata": {},
   "source": [
    "Below is the coarsened version of the holdout, passed to the trained model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0261e9-c11e-4a2f-b9bb-b411baa0fb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ind = 100\n",
    "ecv.plot((coarsened_array[ind][:,:,0], coarsened_array[ind][:,:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51823f-eda9-4b1f-afd5-d27dd98f4ee8",
   "metadata": {},
   "source": [
    "Finally, let's see compare the groundtruth HR t2m and the downscaled t2m obtained with DL4DS for a single time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41b8c6-0e3f-4897-aff9-db871fc1657b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unscaled_y_test = t2m_scaler_train.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3073362-aa89-45aa-adfd-a55b55a30813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecv.plot((unscaled_y_test[ind].values,unscaled_y_pred[ind]), subplot_titles=('groundtruth t2m','downscaled t2m'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e030053-3615-4439-af25-c43df2aedc87",
   "metadata": {},
   "source": [
    "We can also compare the groundtruth HR t2m and the downscaled t2m obtained with DL4DS in a synchronized way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0d135-7ffe-4004-ad02-e47e91c6b9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "( ecv.plot(unscaled_y_test.values) + ecv.plot(unscaled_y_pred, interactive=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a715fff-75a2-4dad-b52f-4479270d8e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
